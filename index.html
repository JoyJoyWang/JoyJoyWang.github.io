<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Ella Zhuxuanzi Wang</title>
  <meta name="description" content="AI Safety research notes on alignment, interpretability, and safety evaluations.">
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <header>
    <div class="header-content">
      <a href="index.html" class="site-title">Ella Zhuxuanzi Wang</a>
      <nav>
        <ul>
          <li><a href="index.html" class="active">Posts</a></li>
          <li><a href="projects.html">Projects</a></li>
          <li><a href="about.html">About</a></li>
        </ul>
      </nav>
    </div>
  </header>

  <main class="container">
    <div class="welcome-section">
      <h1 class="page-title">ðŸ‘‹ Welcome to My Learning Space</h1>
      <p class="page-subtitle">
        Hi, I'm Ella. I'm interested in AI Safety. This space documents my learning journey through foundational papers.
      </p>
      <div class="social-links">
        <a href="https://github.com/zhuxuanziwang" target="_blank" aria-label="GitHub">
          <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.23-.015-2.235-3.015.555-3.795-.735-4.035-1.41-.135-.345-.72-1.41-1.23-1.695-.42-.225-1.02-.78-.015-.795.945-.015 1.62.87 1.845 1.23 1.08 1.815 2.805 1.305 3.495.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.18 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.66 1.65.24 2.88.12 3.18.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.3 0 .315.225.69.825.57A12.02 12.02 0 0024 12c0-6.63-5.37-12-12-12z"/></svg>
        </a>
        <a href="https://x.com/Shiuan_tz" target="_blank" aria-label="X (Twitter)">
          <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
        </a>
        <a href="https://www.linkedin.com/in/ella-w-5205a9375" target="_blank" aria-label="LinkedIn">
          <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
        </a>
      </div>
    </div>

    <!-- Theme View -->
    <div id="theme-view" class="posts-view">
      <div class="theme-section">
        <h2>A. Understanding the Problem</h2>
        <p class="theme-description">Failure modes and research agendas</p>
        <ul class="post-list">
          <li class="post-item">
            <div class="post-meta">
              <span>Date: December 15, 2024</span>
              <span>Reading: 20 min</span>
              <span>ðŸŽ§ Podcast available</span>
            </div>
            <h2 class="post-title">
              <a href="posts/concrete-problems.html">My Notes: Concrete Problems in AI Safety</a>
            </h2>
            <div class="paper-reference">
              <strong>Paper:</strong> <a href="https://arxiv.org/abs/1606.06565" target="_blank">Concrete Problems in AI Safety</a>
              <br><em>Amodei et al., 2016</em>
            </div>
            <p class="post-excerpt">
              Exploring the five categories of AI safety problems: side effects, reward hacking, scalable supervision,
              safe exploration, and distributional shift. A foundational framework for understanding AI safety challenges.
            </p>
            <div class="post-tags">
              <a href="#safety" class="tag">safety</a>
              <a href="#framework" class="tag">framework</a>
            </div>
          </li>

          <li class="post-item">
            <div class="post-meta">
              <span>Date: December 10, 2024</span>
              <span>Reading: 15 min</span>
              <span>ðŸŽ§ Podcast available</span>
            </div>
            <h2 class="post-title">
              <a href="posts/specification-gaming.html">My Notes: Specification Gaming</a>
            </h2>
            <div class="paper-reference">
              <strong>Resource:</strong> <a href="https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/" target="_blank">Specification Gaming: The Flip Side of AI Ingenuity</a>
              <br><em>Google DeepMind</em>
            </div>
            <p class="post-excerpt">
              Understanding how models achieve literal objectives while missing true intent through concrete examples.
              A collection of cases where AI systems find unexpected shortcuts.
            </p>
            <div class="post-tags">
              <a href="#safety" class="tag">safety</a>
              <a href="#reward-hacking" class="tag">reward-hacking</a>
            </div>
          </li>
        </ul>
      </div>

      <div class="theme-section">
        <h2>B. Alignment & Preference Learning</h2>
        <p class="theme-description">RLHF, DPO, and constitutional AI</p>
        <ul class="post-list">
          <li class="post-item">
            <div class="post-meta">
              <span>Date: January 15, 2025</span>
              <span>Reading: 18 min</span>
              <span>ðŸŽ§ Podcast available</span>
            </div>
            <h2 class="post-title">
              <a href="posts/understanding-rlhf.html">My Notes: Understanding RLHF</a>
            </h2>
            <div class="paper-reference">
              <strong>Paper:</strong> <a href="https://arxiv.org/abs/2203.02155" target="_blank">Training language models to follow instructions with human feedback</a>
              <br><em>Ouyang et al., 2022</em>
            </div>
            <p class="post-excerpt">
              My exploration of RLHF from the InstructGPT paperâ€”covering reward modeling, PPO training,
              and practical alignment considerations.
            </p>
            <div class="post-tags">
              <a href="#alignment" class="tag">alignment</a>
              <a href="#rlhf" class="tag">rlhf</a>
            </div>
          </li>
        </ul>
      </div>

      <div class="theme-section">
        <h2>C. Interpretability</h2>
        <p class="theme-description">Mechanistic interpretability and understanding model internals</p>
        <ul class="post-list">
          <li class="post-item">
            <div class="post-meta">
              <span>Date: January 8, 2025</span>
              <span>Reading: 22 min</span>
              <span>ðŸŽ§ Podcast available</span>
            </div>
            <h2 class="post-title">
              <a href="posts/mechanistic-interpretability.html">My Notes: Circuits and Mechanistic Interpretability</a>
            </h2>
            <div class="paper-reference">
              <strong>Paper:</strong> <a href="https://distill.pub/2020/circuits/" target="_blank">Zoom In: An Introduction to Circuits</a>
              <br><em>Olah et al., Distill 2020</em>
            </div>
            <p class="post-excerpt">
              Exploring how to reverse-engineer neural networks through circuit analysis.
            </p>
            <div class="post-tags">
              <a href="#interpretability" class="tag">interpretability</a>
              <a href="#circuits" class="tag">circuits</a>
            </div>
          </li>
        </ul>
      </div>

      <div class="theme-section">
        <h2>D. Security & Adversarial Robustness</h2>
        <p class="theme-description">Red teaming, jailbreaking, adversarial attacks</p>
        <ul class="post-list">
          <li class="post-item">
            <div class="post-meta">
              <span>Date: December 28, 2024</span>
              <span>Reading: 16 min</span>
              <span>ðŸŽ§ Podcast available</span>
            </div>
            <h2 class="post-title">
              <a href="posts/red-teaming-llms.html">My Notes: Red Teaming LLMs</a>
            </h2>
            <div class="paper-reference">
              <strong>Paper:</strong> <a href="https://arxiv.org/abs/2202.03286" target="_blank">Red Teaming Language Models with Language Models</a>
              <br><em>Perez et al., 2022</em>
            </div>
            <p class="post-excerpt">
              My study of automated red teaming methods for discovering LLM failure modes.
            </p>
            <div class="post-tags">
              <a href="#safety" class="tag">safety</a>
              <a href="#red-teaming" class="tag">red-teaming</a>
            </div>
          </li>
        </ul>
      </div>

      <div class="theme-section">
        <h2>E. Evaluation & Governance</h2>
        <p class="theme-description">Safety assessment frameworks for frontier models</p>
        <ul class="post-list">
          <!-- Posts tagged with "governance" theme -->
        </ul>
      </div>
    </div>
  </main>

  <footer>
    <div class="footer-content">
      <ul class="footer-links">
        <li><a href="https://github.com/zhuxuanziwang" target="_blank">GitHub</a></li>
        <li><a href="https://www.linkedin.com/in/ella-w-5205a9375" target="_blank">LinkedIn</a></li>
        <li><a href="https://github.com/zhuxuanziwang/Papers/blob/main/ALICE_Final.pdf" target="_blank">CV</a></li>
      </ul>
      <p>&copy; 2025 Ella Zhuxuanzi Wang</p>
    </div>
  </footer>
</body>
</html>
